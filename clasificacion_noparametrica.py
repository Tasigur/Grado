# -*- coding: utf-8 -*-
"""Clasificacion_NoParametrica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11QNbMrAn0hhFiEtseaxQGBwnqxRHgWLF

# Tema 3. Clasificación no paramétrica

## K-vecinos más próximos

Para seleccionar la arquitectura del modelo a considerar, tendrá que hacer uso de varias topologías y elegir la más adecuada haciendo uso de un conjunto de validación “independiente” de los conjuntos de entrenamiento y test. Explique razonadamente cómo puede construir dicho conjunto para evaluar la “calidad” del aprendizaje conseguido.
   

Diseñe un clasificador k-NN utilizando los subconjuntos de entrenamiento y test.

Incluya la matriz de confusión así como los resultados obtenidos en función de dos medidas de evaluación consideradas.
"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as ss
import seaborn as sbn
import sklearn
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn import linear_model
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import learning_curve, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn import preprocessing
from sklearn import tree
from sklearn.naive_bayes import GaussianNB
from mlxtend.plotting import plot_learning_curves



# Cargar la base de datos
db = pd.read_csv('pima_indian_diabetes.csv')

db['Glucose'] = db['Glucose'].replace(0, np.nan)
db['BloodPressure'] = db['BloodPressure'].replace(0, np.nan)
db['SkinThickness'] = db['SkinThickness'].replace(0, np.nan)
db['Insulin'] = db['Insulin'].replace(0, np.nan)
db['BMI'] = db['BMI'].replace(0, np.nan)
db = db.fillna(np.mean(db))

# Obtenemos X e Y
X = db.drop(['Outcome'], axis = 1)
Y = db['Outcome']
print(X.shape)
print(Y.shape)

# Separamos el conjunto de datos en los conjuntos de train y test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=2)

# Normalizamos los datos
X_norm = preprocessing.scale(X_train)
scaler = preprocessing.StandardScaler().fit(X_train)
X_train_norm = scaler.transform(X_train)
X_test_norm = scaler.transform(X_test)
print(Y_train.shape)
print(X_test_norm.shape)

# k-NN sin cross-validation --> INCORRECTO

# Diseñamos el clasificador KNN, por ejemplo con k = 3
knn = KNeighborsClassifier(n_neighbors = 3)

# Entrenamos el modelo
knn.fit(X_train, Y_train)

## Predecimos usando X_test
y_pred = knn.predict(X_test)

print('Accuracy: ', knn.score(X_test, Y_test))

k_values = range(1,31,2)

print(k_values)

cv_k_scores=[]
for k in k_values:
    kNN=KNeighborsClassifier(n_neighbors=k)
    scores= cross_val_score(kNN, X_train, Y_train, cv=3, scoring='accuracy')
    print('Score',scores)
    cv_k_scores.append(scores.mean())

plt.plot(k_values, cv_k_scores)
plt.xlabel('k')
plt.ylabel('CV accuracy')
plt.show()

# Seleccionamos el máximo valor porque estamso considerando entropía
print(np.array(k_values)[cv_k_scores.index(np.array(cv_k_scores).max())])

kNN=KNeighborsClassifier(n_neighbors=np.array(k_values)[cv_k_scores.index(np.array(cv_k_scores).max())])
model=kNN.fit(X_train,Y_train)

y_pred=model.predict(X_test_norm)

print(accuracy_score(Y_test, y_pred))
print(confusion_matrix(Y_test, y_pred))

# GridSearchCV
param_grid = {'n_neighbors': k_values}
# k-NN
kNN_model=KNeighborsClassifier()

#usamos Gridsearch para Xtest
grid = GridSearchCV(kNN_model, param_grid, cv = 3, scoring = 'accuracy')
grid.fit(X_train,Y_train)
print(grid.best_params_)

my_model = grid.best_estimator_
my_model.fit(X_train_norm, Y_train)
y_predicted = my_model.predict(X_test_norm)
print(accuracy_score(Y_test, y_pred))
print(confusion_matrix(Y_test, y_pred))

from sklearn.model_selection import learning_curve, GridSearchCV
from sklearn.model_selection import validation_curve, GridSearchCV
train_score, val_score = validation_curve(KNeighborsClassifier(), X_train, Y_train,
                                          'n_neighbors', k_values, cv=3)

plt.plot(k_values, np.mean(train_score, 1), color='blue', label='Training accuracy')
plt.plot(k_values, np.mean(val_score, 1), color='red', label='Validation accuracy')
plt.legend(loc='best')
plt.ylim(0, 1.2)
plt.xlabel('k')
plt.ylabel('Accuracy');



"""## Árboles de clasificación

Diseñe un clasificador basado en árboles de decisión utilizando los subconjuntos de entrenamiento y test anteriormente descritos.

Incluya la matriz de confusión así como los resultados obtenidos en función de dos medidas de evaluación consideradas.
"""

# Parámetros libres
max_depth = np.arange(1, 21)
min_samples_leaf = [50, 100, 200]

# GridSearchCV
param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(3, 15)}

# Creamos un árbol de clasificación
dtree_model=tree.DecisionTreeClassifier()

# Usamos gridsearch para evluar los parámetros
dtree_model = GridSearchCV(dtree_model, param_grid, cv=3, scoring="accuracy")

#
dtree_model=dtree_model.fit(X_train, Y_train)
print(dtree_model.best_params_)



# GridSearchCV
param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(3, 15)}

# Creamos un árbol de clasificación
dtree_model=tree.DecisionTreeClassifier()

# Usamos gridsearch para evluar los parámetros
dtree_model = GridSearchCV(dtree_model, param_grid, cv=3, scoring="accuracy")

#
dtree_model=dtree_model.fit(X_train_norm, Y_train)
print(dtree_model.best_params_)

# Entrenamos el modelo
my_model = dtree_model.best_estimator_
my_tree=my_model.fit(X_train, Y_train)

#Predeccimos usando X_test
y_predicted = my_model.predict(X_test)

# Resultados
print(accuracy_score(Y_test, y_predicted))
print(confusion_matrix(Y_test, y_predicted))

# Entrenamos el modelo
my_model = dtree_model.best_estimator_
my_tree=my_model.fit(X_train_norm, Y_train)

#Predeccimos usando X_test
y_predicted = my_model.predict(X_test_norm)

# Resultados
print(accuracy_score(Y_test, y_predicted))
print(confusion_matrix(Y_test, y_predicted))

#Evaluamos la profundidad del árbol
my_tree.get_depth()

# Obtenemos los valores del árbol
my_tree.get_params()

# Representamos cuál son las características más relevantes
from matplotlib import pyplot
importance=my_tree.feature_importances_
pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()

from sklearn.tree import export_graphviz
#from sklearn.externals.six import StringIO
from six import StringIO
from IPython.display import Image
import pydotplus

from sklearn.model_selection import learning_curve, GridSearchCV
#from sklearn.learning_curve import validation_curve
from sklearn.model_selection import validation_curve, GridSearchCV
train_score, val_score = validation_curve(tree.DecisionTreeClassifier(), X_train_norm, Y_train,
                                          'max_depth', max_depth, cv=3)

plt.plot(max_depth, np.mean(train_score, 1), color='blue', label='Training accuracy')
plt.plot(max_depth, np.mean(val_score, 1), color='red', label='Validation accuracy')
plt.legend(loc='best')
plt.ylim(0.5, 1.2)
plt.xlabel('Depth')
plt.ylabel('Accuracy');



dot_data = StringIO()
export_graphviz(my_tree, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True,feature_names = db.columns[0:-1],class_names=['Diabetic','Non Diabetic'])

graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('diabetes.png')
Image(graph.create_png())

"""# Entregable

## K-NN
Considere la normalización de características del espacio original de entrada al modelo. Justifique el tipo de normalización considerada y explique cómo lo realiza sobre cada subconjunto (entrenamiento, validación y test). Indique si las prestaciones obtenidas en el conjunto de test cambian tras normalizar las variables.


Explique razonadamente la relación entre la capacidad de generalización y el valor de k. Incluya las curvas de obtenidas para una medida de evaluación considerada tanto para el conjunto de train como para el de validación en función del parámetro k. Coméntelas brevemente

**Estandarización.**

Normalizar las características para evitar que una de ellas domine el cálculo de las distancias.Estos algoritmos basados en medidas de distancia
son muy sensibles a la presencia de atributos con distinto rango dinámico.

Cada subconjunto de características (X) se normaliza a parte, teniendo en cuenta la media y la desvicación típica de cada uno. Así se evita entrenar al modelo con datos ajenos a los subconjuntos
entrenamiento.
Los subconjuntos que contienen la clase (Y) no se normalizan.

Si entrenamos el modelo sin normalizar, la k propuesta por el codigo es 19, mientras que una vez normalizados los datos, la k óptima es 13.

Para valores muy bajos de k, se generarán fronteras muy complejas y se producirá sobreajuste.Por otro lado,un valor demasiado alto de k las hará mas suaves, con lo que se generalizará de forma errónea.El objetivo es encontrar un valor de k que aproxime de forma correcta las nuevas observaciones.
"""

from sklearn.model_selection import learning_curve, GridSearchCV
from sklearn.model_selection import validation_curve, GridSearchCV
train_score, val_score = validation_curve(KNeighborsClassifier(), X_train_norm, Y_train,
                                          'n_neighbors', k_values, cv=3)

plt.plot(k_values, np.mean(train_score, 1), color='blue', label='Training accuracy')
plt.plot(k_values, np.mean(val_score, 1), color='red', label='Validation accuracy')
plt.legend(loc='best')
plt.ylim(0, 1.2)
plt.xlabel('k')
plt.ylabel('Accuracy');

y_pred=model.predict(X_test_norm)
s3=accuracy_score(Y_test, y_pred)
s3

"""## Árboles de decisión


Indique si las prestaciones obtenidas en el conjunto de test cambian tras normalizar las variables.

En términos de subajuste y sobreajuste, explique cuál de ellos es más probable que ocurra si el máximo número de casos por nodo del árbol es muy pequeño.

Indique cuáles de los parámetros comentados en teoría ha analizado para conseguir que el modelo diseñado tenga capacidad de generalización. Incluya las curvas de obtenidas para una medida de evaluación considerada tanto para el conjunto de train como para el de validación en función del parámetro k. Coméntelas brevemente.

En el siguiente código se puede ver como, tras normalizar los datos, el modelo obtenido tiene las mismas prestaciones que si no se realizara la normalización previamente.
"""

my_model = dtree_model.best_estimator_
my_tree=my_model.fit(X_train_norm, Y_train)


y_predicted = my_model.predict(X_test_norm)

print(accuracy_score(Y_test, y_predicted))
print(confusion_matrix(Y_test, y_predicted))

my_model = dtree_model.best_estimator_
my_tree=my_model.fit(X_train, Y_train)

y_predicted = my_model.predict(X_test)
s2=accuracy_score(Y_test, y_predicted)
print(s2)
print(confusion_matrix(Y_test, y_predicted))

"""Se produciría sobreajuste (poca generalización) ya que teniendo pocos casos en cada nodo, el número total de nodos sería alto.

Cada nodo contará con unos casos con características muy concretas, por ello se produce una mala generalización

Los parametros tenidos en cuenta son:

-Mínimo número de muestras para un nodo terminal; si este número es demasiado bajo sobreajustaremos demasiado

-Mínimo número de muestras para dividir un nodo; para no dividir un nodo que ya está lo suficiente ajustado.
"""

from sklearn.model_selection import learning_curve, GridSearchCV
#from sklearn.learning_curve import validation_curve
from sklearn.model_selection import validation_curve, GridSearchCV
train_score, val_score = validation_curve(tree.DecisionTreeClassifier(), X_train_norm, Y_train,
                                          'max_depth', max_depth, cv=3)

plt.plot(max_depth, np.mean(train_score, 1), color='blue', label='Training accuracy')
plt.plot(max_depth, np.mean(val_score, 1), color='red', label='Validation accuracy')
plt.legend(loc='best')
plt.ylim(0.5, 1.2)
plt.xlabel('Depth')
plt.ylabel('Accuracy');

from sklearn.model_selection import learning_curve, GridSearchCV
#from sklearn.learning_curve import validation_curve
from sklearn.model_selection import validation_curve, GridSearchCV
train_score, val_score = validation_curve(tree.DecisionTreeClassifier(), X_train, Y_train,
                                          'max_depth', max_depth, cv=3)

plt.plot(max_depth, np.mean(train_score, 1), color='blue', label='Training accuracy')
plt.plot(max_depth, np.mean(val_score, 1), color='red', label='Validation accuracy')
plt.legend(loc='best')
plt.ylim(0.5, 1.2)
plt.xlabel('Depth')
plt.ylabel('Accuracy');

"""## Perceptrón Multicapa

Haciendo uso de las características del conjunto de observaciones en el espacio original, diseñe un Perceptrón Multicapa (MLP) con una única capa oculta, inicialización aleatoria de los pesos de la red neuronal, aprendizaje en modo mini-batch (por mini-lotes), algoritmo de entrenamiento back- propagation y detención del aprendizaje haciendo uso de la técnica “early stopping”.
Justifique razonadamente:
- El número de neuronas de entrada al MLP.
- El número de neuronas de salida del MLP.
- Las funciones de activación consideradas.
- El valor de la tasa de aprendizaje utilizada, así como variación (o no) de la misma durante el
aprendizaje.
- Función de coste a optimizar.
- Número máximo de épocas. Explique de manera breve y concisa qué es una época,
ejemplificando con su caso particular.
- Los valores numéricos que representan el número de neuronas consideradas en la capa oculta.
Justifique la consideración de cada uno de los valores indicados (al menos, tres valores:
n_hidden_1, n_hidden_2 y n_hidden_3).



- O Porcentaje de observaciones consideradas al aplicar la técnica “early stopping”.
- O Tamaño del mini-batch.
- O Si hace uso o no (y por qué) de aleatorización de las observaciones durante el aprendizaje.
Realice el aprendizaje de todas las topologías, i.e., considerando al menos n_hidden_1, n_hidden_2 y n_hidden_3 neuronas en la capa oculta. Elija varias medidas de prestaciones (adecuadas para la tarea a resolver) y construya una tabla indicando, para cada topología del MLP, dichas prestaciones sobre el conjunto de validación. Justifique razonadamente la elección de una topología y obtenga las prestaciones sobre el conjunto de test.

Habrá:

- 8 neuronas de entrada, una por cada característica

- Solo una de salida, ya que es un modelo de clasficicación binaria y es suficiente.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split


clf = MLPClassifier(hidden_layer_sizes=(100,),random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)
clf.n_layers_

"""**Funciones:**

-Logistica
"""

clf = MLPClassifier(hidden_layer_sizes=(100,),activation='logistic',random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""-Tanh"""

clf = MLPClassifier(hidden_layer_sizes=(100,),activation='tanh',random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

y_pred= clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""ReLu"""

clf = MLPClassifier(hidden_layer_sizes=(100,),activation='relu',random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

y_pred= clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""-F(x)=x

"""

clf = MLPClassifier(hidden_layer_sizes=(100,),activation='identity',random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""**Tasa de aprendizaje**

-Constante
"""

clf = MLPClassifier(hidden_layer_sizes=(100,),activation='relu',random_state=1,early_stopping=True,learning_rate='constant', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""-Escalado inverso"""

clf = MLPClassifier(hidden_layer_sizes=(100,),activation='relu',random_state=1,early_stopping=True,learning_rate='invscaling', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""-Adaptativo"""

clf = MLPClassifier(hidden_layer_sizes=(100,),activation='relu',random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""**Funcion de coste**

La función de coste a optimizar será la de pérdida logística(log_loss), ya que en problemas de clasificación binaria es más adecuado estudiar esta, y no la entropía cruzada.

Así, a menor valor de log_loss, mejor *accuracy* tendrá modelo.
"""

clf = MLPClassifier(hidden_layer_sizes=(100,),activation='relu',random_state=1,early_stopping=True,learning_rate='constant', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

y_pred=clf.predict_proba(X_test_norm)

clf.score(X_test_norm, Y_test)

sklearn.metrics.log_loss(Y_test,y_pred)

"""**Épocas**

Una época consiste en un ciclo de entrenamiento completo en el conjunto de entrenamiento. Una vez que se ven todas las muestras del conjunto, comienza de nuevo, marcando el comienzo de la segunda época.

Por defecto, el número de épocas del *MLPClassifier* (usando el solver "adam") es 200.

(Parámetro *max_iter* = 200)

**Número de neuronas**

- 100
"""

clf = MLPClassifier(hidden_layer_sizes=(100,),random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

s1=clf.score(X_test_norm, Y_test)
s1

"""- 500"""

clf = MLPClassifier(hidden_layer_sizes=(500,),random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""- 2"""

clf = MLPClassifier(hidden_layer_sizes=(2,),random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""- 50"""

clf = MLPClassifier(hidden_layer_sizes=(50,),random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""- 200

"""

clf = MLPClassifier(hidden_layer_sizes=(200,),random_state=1,early_stopping=True,learning_rate='adaptive', max_iter=300).fit(X_train_norm, Y_train)
clf.predict_proba(X_test_norm)

clf.predict(X_test_norm)

clf.score(X_test_norm, Y_test)

"""## Comparación de resultados

A la vista de los resultados obtenidos para resolver la tarea de clasificación planteada, justifique brevemente qué modelo considera más adecuado.
"""

Ta={'KNN':[s3],'Árbol de decisión':[s2],'MLP(ReLu)(Nºneuronas C.O. =100 )(Nº C.O.=1)':[s1]}
Ta1= pd.DataFrame(Ta,index=["accuracy"])
Ta1

