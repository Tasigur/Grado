# -*- coding: utf-8 -*-
"""MineriaTexto_DBmedica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ulhfw9kaJUJZkuZyGW-3sEKzTdYBAzo
"""

import pandas as pd
import numpy as np
import seaborn as sns
import scipy
import matplotlib
from matplotlib import pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import re
import string
from scipy import stats as ss

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_excel("/content/drive/MyDrive/INFORMES.xlsx")
data.head()
col_name=list(data.columns)
data.columns = ['infeccion', 'ID',"texto"]

plt.hist(data["infeccion"])

data["infeccion"].value_counts()

"""Como podemos comprobar en las dos celdas anteriores, las clases se encuentran completamente desbalanceadas."""

def clean_text_round1(text):
    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''
    text = text.lower()
    text = re.sub('\[.*?¿\]\%', ' ', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)
    text = re.sub('\w*\d\w*', '', text)
    return text
def clean_text_round2(text):
    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''
    text = re.sub('[‘’“”…«»]', '', text)
    text = re.sub('\n', ' ', text)
    return text
def eliminar_tilde(text):
    text=text.replace("á","a")
    text=text.replace("é","e")
    text=text.replace("í","i")
    text=text.replace("ó","o")
    text=text.replace("ú","u")
    return text

for k in range(0,len(data['texto'])):
    clean1=clean_text_round1(data['texto'][k])
    clean2=clean_text_round2(clean1)
    data['texto'][k]=eliminar_tilde(clean2)

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords


texto = data.texto.str.cat(sep=' ')

#function to split text into word
tokens = word_tokenize(texto)

vocabulary = set(tokens)
print(len(vocabulary))


stop_words = set(stopwords.words('spanish'))
print(stop_words)
tokens = [w for w in tokens if not w in stop_words]
frequency_dist = nltk.FreqDist(tokens)
sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]

X=data["texto"]
y=data["infeccion"]
X_train, X_test, y_train, y_test = train_test_split(
    X, y,stratify=y, test_size=0.35, random_state=42)

"""Aquí tomamos como variable de salida (y) los datos recogidos en la columna "infeccion" y como variable de entrada (x) los datos recogidos en la columna "texto"
"""

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
train_vectors = vectorizer.fit_transform(X_train)
test_vectors = vectorizer.transform(X_test)
print(train_vectors.shape, test_vectors.shape)

from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB

clf =  MultinomialNB().fit(train_vectors, y_train)
predicted = clf.predict(test_vectors)

print(accuracy_score(y_test,predicted))
print(confusion_matrix(y_test,predicted))

clf =  SGDClassifier().fit(train_vectors, y_train)
predicted = clf.predict(test_vectors)

print(accuracy_score(y_test,predicted))
print(confusion_matrix(y_test,predicted))





"""Dado que la variable de entrada es texto libre, debemos "vectorizarla" para poder trabajar con ella. Para ello usamos la funcion "TfidfVectorizer" de "sklearn"
"""

from imblearn.over_sampling import SMOTE

X_resample, y_resampled = SMOTE().fit_resample(train_vectors, y_train)

"""Como ya vimos, las clases estaban desbalanceadas. Para solucionar esto, usamos Smote ; lo cual nos permite corregir este desbalanceo."""

from sklearn.linear_model import SGDClassifier

clf = SGDClassifier().fit(X_resample, y_resampled)

"""Usamos SGDClassifier para crear el modelo predictor con los datos de training."""

from sklearn.metrics import accuracy_score

predicted = clf.predict(test_vectors)

print(accuracy_score(y_test,predicted))

print(confusion_matrix(y_test,predicted))

print(classification_report(y_test,predicted))

"""Evaluamos nuestro modelo usando los datos de test.

######################################################################################################
"""









data["texto"]

id_infeccion=[]
posicion=[]
textoinforme=[]
INFECCION=[]
cont=0

for x in data["texto"]:
    s,t = 'áéíóúü','aeiouu'
    trans = str.maketrans(s,t)
    x=x.lower()
    x=x.translate(trans)
    cont=cont+1
    a=x.find("infeccion")
    b=x.find("sepsis")
    c=x.find("absceso")

    if a != -1:
        id_infeccion.append(data["ID"][cont-1])
        posicion.append(cont)
        textoinforme.append(data["texto"][cont-1])
        INFECCION.append(data["infeccion"][cont-1])

    elif b != -1:
        id_infeccion.append(data["ID"][cont-1])
        posicion.append(cont)
        textoinforme.append(data["texto"][cont-1])
        INFECCION.append(data["infeccion"][cont-1])


    elif c != -1:
        id_infeccion.append(data["ID"][cont-1])
        posicion.append(cont)
        textoinforme.append(data["texto"][cont-1])
        INFECCION.append(data["infeccion"][cont-1])

dat=pd.DataFrame({"Posición":posicion,"ID":id_infeccion ,"Texto":textoinforme,"INFECCION":INFECCION})
dat
# Todos los informes en los que se menciona "infección","sepsis" o "absceso"
# con sus respectivos ID y posición en la tabla original

cont=0
todo=[]
for i in dat["Texto"]:
    comp=0
    nueva=[]
    juntas=[]

    s,t = 'áéíóúü','aeiouu'
    trans = str.maketrans(s,t)
    i=i.lower()
    i=i.translate(trans)
    i.replace(" ","")
    cont=cont+1
    a=i.find("infeccion")
    b=i.find("sepsis")
    c=i.find("absceso")
    if a != -1:
        if comp==0:
            comp=comp+1
            for z in range(-50,20):
                nueva.append(i[a+z])
    elif b != -1:
        if comp==0:
            comp=comp+1
            for z in range(-50,20):
                nueva.append(i[b+z])
    elif c != -1:
        if comp==0:
            comp=comp+1
            for z in range(-50,20):
                nueva.append(i[c+z])
    juntas="".join(nueva)
    todo.append(juntas)

dat=pd.DataFrame({"Posición":posicion,"ID":id_infeccion ,"Texto":todo,"INFECCION":INFECCION})
dat

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords


texto = dat.Texto.str.cat(sep=' ')

#function to split text into word
tokens = word_tokenize(texto)

vocabulary = set(tokens)
print(len(vocabulary))


stop_words = set(stopwords.words('spanish'))

tokens = [w for w in tokens if not w in stop_words]
frequency_dist = nltk.FreqDist(tokens)
sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]

X_train = dat.loc[:35, 'Texto'].values
y_train = dat.loc[:35, 'INFECCION'].values
X_test = dat.loc[36:, 'Texto'].values
y_test = dat.loc[36:, 'INFECCION'].values

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
train_vectors = vectorizer.fit_transform(X_train)
test_vectors = vectorizer.transform(X_test)
print(train_vectors.shape, test_vectors.shape)

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB().fit(train_vectors, y_train)

from sklearn.metrics import accuracy_score

predicted = clf.predict(test_vectors)

print(accuracy_score(y_test,predicted))

print(classification_report(y_test,predicted))

