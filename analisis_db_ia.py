# -*- coding: utf-8 -*-
"""Analisis_DB_IA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ngzX-LYI1dd6KxK5wwEZIlvK9pJWbILy
"""

import pandas as pd
import seaborn as sns
import scipy
import matplotlib
from sklearn.metrics import accuracy_score
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt
import math
import sympy
from sklearn.model_selection import KFold
from scipy import stats
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn import linear_model
from sklearn.linear_model import LinearRegression
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import sklearn

data = pd.read_csv("Somerville.csv")
data.columns= ["Outcome","City services","Housing","Public Schools","Police","Streets","Events"]
col_name=list(data.columns)
col_name_2=list(["City services","Housing","Public Schools","Police","Streets","Events"])
data2=data.drop(["Outcome"],axis=1)
Outcome=data["Outcome"]

"""Esta base de datos proviene de una encuesta en la que a los ciudadanos de Somerville se les pedía que otorgasen una puntuación
del 1  al 5 (siendo 5 la puntuación más alta) a diferentes aspectos de la ciudad. Tras ello, se les preguntaba si eran o no felices. Con ello se pretende ver como influye la ciudad en la felicidad de las personas.

- Outcome= 1 (feliz) 0 (infeliz)
- City services = disponibilidad de información sobre los servicios urbanos
- Housing = coste inmobiliario
- Public Schools = calidad de las escuelas públicas
- Police = confianza en la policía local
- Streets = mantenimiento de calles y paseos
- Events = disponibilidad de eventos comunitarios

## PRÁCTICA 1
"""

for columna in col_name_2:
    plt.hist(data2[columna])
    plt.title(columna)
    plt.show()

"""En esta base de datos, un espacio en blanco o un "NA" indica que esa pregunta no le fue realizada a esa persona , mientras que un "999" indica que la persona decidió no responder a la pregunta.

Como se ve en la representación no aparece ningun 999.
"""

for a in col_name_2:
    print(data2[a].value_counts())

data2.isnull().sum()

"""Aquí comprobamos que no hay ningún "NA", ningún espacio en blanco ni ningún 999.

Al estar todas las características en el mismo rango dinámico (1-5) no será necesario normalizar y esto facilitirá el estudio de la base más adelante
"""

plt.hist(data["Outcome"])

data["Outcome"].value_counts()

"""Como podemos ver en esta última gráfica las muestras no estás desbalanceadas, ambas salidas posibles tienen aproximadamente el mismo número de muestras ( 77 vs 66 ).

## PRÁCTICA 2

Como contamos con las etiquetas de la variable de salida "Outcome" vamos a utilizar metodos de aprendizaje supervisado para generar clasificadores a partir de los datos que tenemos.

Dado que se trata de un problema de clasificación estudiaremos modelos creados utilizando  :
- regresión logística (clasificación paramétrica)
- k-nn  (clasificación no paramétrica)
- árboles de decisión (clasificación no paramétrica)
- perceptron multicapa (clasificación no paramétrica)
"""

import seaborn as sns

f, ax = plt.subplots(figsize=(10, 8))
corr = data.corr()
sns.heatmap(corr,annot=True,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)
plt.show()

"""Aquí obtenemos las correlaciones existentes entre las distintas variables.Por ejemplo, vemos que:
- City services y Events tienen la mayor correlación de todas, por lo que están bastante relacionadas.
- Streets y Housing tienen la menor correlación entre ellas, por lo que la relación existente entre ellas es practicamente inexistente.

Los modelos que estudiaremos los basaremos en estas correlaciones, evaluando si es mejor usar todas las características o solo aquellas más correladas.

En primer lugar, vamos a dividir en conjuntos de train y test.

Aquí utilizamos todas las características:
"""

X=data2
y=Outcome

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=0)
n=len(y_test)

model = linear_model.LogisticRegression()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""Ahora repetimos el proceso, pero quitando la que tiene la correlacion más baja con Outcome . ( Housing )"""

X=data2.drop(["Housing"],axis=1)
y=Outcome

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=0)
n=len(y_test)
model = linear_model.LogisticRegression()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""Ahora repetimos el proceso, pero quitando la que tiene la correlacion más alta con Outcome. ( City services)"""

X=data2.drop(["City services"],axis=1)
y=Outcome

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=0)
n=len(y_test)
model = linear_model.LogisticRegression()
model.fit(X_train,y_train)
y_pred=model.predict(X_test)

print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""Hemos comprobado que quitar la variable con mayor correlación (City services) afecta y empeora más las figuras de mérito del predictor en comparación con lo que afecta quitar la variable con menor correlación (Housing).

También hemos comprobado que no merece la pena quitar ninguna característica ya que esto empeora nuestro modelo.

## Práctica 3
"""

from sklearn import tree
from sklearn.model_selection import learning_curve, GridSearchCV
from sklearn.model_selection import validation_curve, GridSearchCV
max_depth = np.arange(1, 21)
min_samples_leaf = [50, 100, 200]
# GridSearchCV
param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(1,9)}

# Creamos un árbol de clasificación
dtree_model=tree.DecisionTreeClassifier()

# Usamos gridsearch para evluar los parámetros
dtree_model = GridSearchCV(dtree_model, param_grid, cv=3, scoring="accuracy")

#
dtree_model=dtree_model.fit(X_train, y_train)
print(dtree_model.best_params_)

my_model = dtree_model.best_estimator_
my_tree=my_model.fit(X_train, y_train)

my_tree

from sklearn.tree import export_graphviz
#from sklearn.externals.six import StringIO
from six import StringIO
from IPython.display import Image
import pydotplus

dot_data = StringIO()
export_graphviz(my_tree, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True,feature_names = data2.columns[0:-1],class_names=['Infeliz','Feliz'])

graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('somerville.png')
Image(graph.create_png())

